from modal import Image, App, Secret,Mount
import torch
import sys
import torch
import os
from omegaconf import OmegaConf
from huggingface_hub import login
app = App("llama3-test")

# Use the same docker image
image = Image.from_dockerfile("Dockerfile")
# yaml_path = "/llm-foundry/scripts/train/yamls/llama/llama3-1b-lora.yaml"
# print(f"Loading config from {yaml_path}")
# cfg = OmegaConf.load(yaml_path)
# print("Config loaded successfully")
# Mount your local YAML directory
# yaml_mount = Mount.from_local_dir(
#     local_path="/home/mainuser/Desktop/llm-foundry/scripts/train/yamls/llama",  # Full path
#     remote_path="/llm-foundry/scripts/train/yamls/llama"  # Path in container
# )

# Mount both the YAML directory and your llama module code
image = image.add_local_dir(
    local_path="/home/mainuser/Desktop/llm-foundry/scripts/train/yamls/llama", 
    remote_path="/llm-foundry/scripts/train/yamls/llama"
)

# Mount your custom llama module implementation
image = image.add_local_dir(
    local_path="/home/mainuser/Desktop/llm-foundry/llmfoundry/models/llama", 
    remote_path="/llm-foundry/llmfoundry/models/llama"
)

# Make sure __init__.py file exists
image = image.add_local_file(
    local_path="/home/mainuser/Desktop/llm-foundry/llmfoundry/models/__init__.py",
    remote_path="/llm-foundry/llmfoundry/models/__init__.py"
)


image = image.add_local_dir(
    local_path="/home/mainuser/Desktop/llm-foundry/scripts/train/custom_plugin", 
    remote_path="/llm-foundry/scripts/train/custom_plugin"
)


def build_llama(cfg):
    """Create the Python script content for testing the Llama model."""
    


    sys.path.append('/llm-foundry')
    hf_token = os.environ.get("HUGGINGFACE_TOKEN")
    if hf_token:
        print("Logging in to Hugging Face...")
        login(token=hf_token)
    else:
        print("WARNING: No Hugging Face token found, access to gated models may fail.")
    try:

        # Import our custom Llama implementation
        from llmfoundry.models.llama import LlamaForCausalLM


        # Build model from config
        print("Building model...")
        model_cfg = dict(cfg.model)
        model = LlamaForCausalLM.from_config(model_cfg)
        print(f"Model built successfully - {model.__class__.__name__}")

        # Count parameters
        total_params = sum(p.numel() for p in model.parameters())
        print(f"Model has {total_params/1e6:.2f}M parameters")

        # Move to GPU if available
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Moving model to {device}...")
        model = model.to(device)

        # Convert to bfloat16 for Flash Attention
        print("Converting model to bfloat16...")
        model = model.to(torch.bfloat16)

        print("Model conversion successful")
        return model

    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        raise ValueError("Invalid Model Matching")



def llama_generate(model,cfg):
    print("Testing initial custom loaded model generation")
    from transformers import AutoTokenizer
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    # Initialize tokenizer
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(cfg.variables.tokenizer_name)

    print("Tokenizer loaded")
    # Set model to evaluation mode
    model.eval()

    # Define prompts
    prompts = [
        "Explain machine learning to a 10-year old:",
        "Write a short poem about artificial intelligence:",
        "What are the key benefits of using PyTorch for deep learning?"
    ]

    # Generate text for each prompt
    print("\\nGenerating responses...")
    for i, prompt in enumerate(prompts):
        print(f"\\n[Prompt {i+1}]\\n{prompt}\\n")
        
        # Tokenize
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # Generate
        with torch.no_grad():
            try:
                output_ids = model.generate(
                    inputs.input_ids,
                    max_new_tokens=256,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True
                )
                
                # Decode and print
                generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
                response = generated_text[len(prompt):].strip()
                print(f"[Response]\\n{response}\\n")
                print("-" * 50)
            except Exception as e:
                print(f"Error generating response: {e}")
                import traceback
                traceback.print_exc()   



################
def create_combined_script():
    """Create a script that properly initializes HuggingFaceModel
    Using our custom architecture (LlamaForCausalLM) with whatever special optimizations or modifications it has
    Loading weights into our custom model from the pretrained checkpoint
    Mimicking the expected interface by creating a wrapper class that matches what Composer expects 
    (ComposerHFCausalLM)
    This is a classic adapter pattern in software design. 
    Instead of modifying the framework (which would be difficult and intrusive), 
    we're making our custom implementation look like what the framework expects.
    """
    script_path = "/llm-foundry/scripts/train_custom_loss.py"
    
    with open(script_path, "w") as f:
        f.write("""
import sys
import os
import torch
import inspect
from composer.models import HuggingFaceModel, ComposerModel
from llmfoundry.models.llama import LlamaForCausalLM
from llmfoundry import registry
from llmfoundry.command_utils import train_from_yaml
from transformers import AutoTokenizer, AutoConfig, LlamaForCausalLM as HFLlamaForCausalLM

# Add paths to Python path
sys.path.insert(0, '/llm-foundry')
sys.path.insert(0, '/llm-foundry/scripts')

# Add the missing method from HuggingFace's implementation to our custom implementation
if not hasattr(LlamaForCausalLM, 'prepare_inputs_for_generation'):
    print("Adding prepare_inputs_for_generation method to our custom LlamaForCausalLM")
    prepare_inputs_for_generation = HFLlamaForCausalLM.prepare_inputs_for_generation
    LlamaForCausalLM.prepare_inputs_for_generation = prepare_inputs_for_generation
    print("Successfully added method")

# Create our custom class that uses LlamaForCausalLM
try:
    from llmfoundry.models.hf.hf_causal_lm import ComposerHFCausalLM
    from peft import LoraConfig
    
    # Create a proper subclass with exact signature match
    class CustomLlamaModel(ComposerHFCausalLM):
        def __init__(self, 
                    pretrained_model_name_or_path,
                    tokenizer=None,
                    config=None,
                    use_logits=False,
                    metrics=None,
                    eval_metrics=None,
                    shift_labels=None,
                    allow_embedding_resizing=False,
                    peft_config=None,
                    should_save_peft_only=True,
                    init_device='meta',
                    **kwargs):
            print("Initializing CustomLlamaModel")
            
            # Handle import_path and any other unwanted kwargs
            if 'import_path' in kwargs:
                del kwargs['import_path']
            
            # Make sure we're using a string for model path
            if not isinstance(pretrained_model_name_or_path, str):
                print(f"WARNING: pretrained_model_name_or_path is not a string: {type(pretrained_model_name_or_path)}")
                if hasattr(pretrained_model_name_or_path, 'name_or_path'):
                    pretrained_model_name_or_path = pretrained_model_name_or_path.name_or_path
                else:
                    pretrained_model_name_or_path = "meta-llama/Llama-3.2-1B"
            
            print(f"Loading model from: {pretrained_model_name_or_path}")
            
            # Load configuration separately
            print("Loading model config...")
            if config is None:
                config = AutoConfig.from_pretrained(pretrained_model_name_or_path)
                print(f"Loaded config: {type(config)}")
            
            # Use our custom LlamaForCausalLM implementation
            model = LlamaForCausalLM.from_pretrained(
                pretrained_model_name_or_path, 
                torch_dtype=torch.bfloat16,
                config=config,
                **kwargs
            )
            
            print(f"Using custom LlamaForCausalLM: {type(model).__name__}")
            
            # Ensure model has a config attribute
            if not hasattr(model, 'config'):
                print("Adding config attribute to model")
                model.config = config
            
            # Convert peft_config dict to proper PEFT config object
            if peft_config is not None and isinstance(peft_config, dict):
                print(f"Converting peft_config dict to proper PEFT config: {peft_config}")
                
                # Handle terminology differences
                if 'peft_type' not in peft_config and 'peft_method' in peft_config:
                    peft_config['peft_type'] = peft_config['peft_method']
                    del peft_config['peft_method']
                
                # Check if it's a LoRA config
                if peft_config.get('peft_type', '').upper() == 'LORA':
                    # Create proper LoraConfig
                    lora_config = {
                        'r': peft_config.get('r', 8),
                        'lora_alpha': peft_config.get('lora_alpha', 16),
                        'lora_dropout': peft_config.get('lora_dropout', 0.05),
                        'target_modules': peft_config.get('target_modules', ['q_proj', 'k_proj', 'v_proj', 'o_proj']),
                        'bias': 'none',
                        'task_type': peft_config.get('task_type', 'CAUSAL_LM'),
                        'base_model_name_or_path': pretrained_model_name_or_path
                    }
                    print(f"Creating LoraConfig with: {lora_config}")
                    peft_config = LoraConfig(**lora_config)
                else:
                    print(f"Warning: Unsupported PEFT type: {peft_config.get('peft_type')}")
            
            # Initialize ComposerModel base
            ComposerModel.__init__(self)
            
            # Create the HuggingFaceModel wrapper
            hf_model = HuggingFaceModel(
                model=model,
                tokenizer=tokenizer,
                use_logits=use_logits,
                metrics=metrics,
                eval_metrics=eval_metrics,
                shift_labels=shift_labels,
                allow_embedding_resizing=allow_embedding_resizing,
                peft_config=peft_config,
                should_save_peft_only=should_save_peft_only
            )
            
            # Store the original model specifically
            self.hf_model = hf_model
            self.model = hf_model.model
            
            # Copy all attributes and methods from hf_model to self
            for key, value in vars(hf_model).items():
                if key not in ['hf_model', 'model']:  # Avoid circular references
                    setattr(self, key, value)
            
            for name in dir(hf_model):
                if not name.startswith('_'):
                    attr = getattr(hf_model, name)
                    if callable(attr) and not hasattr(self, name):
                        setattr(self, name, attr)
                            
            print("CustomLlamaModel initialization complete")
        
        # Implement a custom loss function that handles both dict and tuple outputs
        def loss(self, outputs, batch):
            print(f"CustomLlamaModel.loss called with outputs type: {type(outputs)}")
            
            if isinstance(outputs, dict):
                if 'loss' in outputs:
                    return outputs['loss']
            elif isinstance(outputs, tuple):
                # Assuming the loss is the first element in the tuple if present
                if len(outputs) > 0 and outputs[0] is not None:
                    return outputs[0]
            
            # If we can't find the loss in the outputs, try to compute it
            # This is a fallback and might not be needed if your model returns loss correctly
            print("Computing loss from logits and labels")
            
            logits = outputs['logits'] if isinstance(outputs, dict) else outputs[0]
            labels = batch['labels'] if isinstance(batch, dict) else batch[1]
            
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            
            # Flatten the tokens
            loss_fct = torch.nn.CrossEntropyLoss()
            vocab_size = getattr(self.model, 'vocab_size', shift_logits.size(-1))
            loss = loss_fct(shift_logits.view(-1, vocab_size), shift_labels.view(-1))
            
            return loss
    
    # Register our class
    registry.models.register("hf_causal_lm")(CustomLlamaModel)
    print("Registered CustomLlamaModel as hf_causal_lm")
except Exception as e:
    print(f"Error creating CustomLlamaModel: {e}")
    import traceback
    traceback.print_exc()

# Parse command line arguments
yaml_path = sys.argv[1]
data_path = sys.argv[2]

print(f"Running training with YAML: {yaml_path}")
print(f"Data path: {data_path}")

# Call the train function directly in this process
print("Starting training...")
args_list = [f"variables.data_local={data_path}"]
train_from_yaml(yaml_path, args_list)
""")
    
    return script_path

###############




@app.function(
    gpu="a100", #"l4"
    image=image, 
    timeout=3600, 
    secrets=[Secret.from_name("LRG"), Secret.from_name("huggingface-secret")],
    #env={"HF_TOKEN": "{{secret:huggingface-secret.HUGGINGFACE_TOKEN}}"} # Set as environment variable
)
def run_llama():
    import subprocess
    import os
    # Get token from secret and set environment variable
    # Debug environment variables from the secret
    print("Environment variables starting with HUGGINGFACE:")
    for key in os.environ:
        if "HUGGINGFACE" in key:
            print(f"  {key}")
    
    # Try various possible names for the token
    hf_token =  os.environ.get("HF_TOKEN") 
      
    
    if hf_token:
        # Use found token
        os.environ["HUGGINGFACE_TOKEN"] = hf_token
        os.environ["HF_TOKEN"] = hf_token
        os.environ["HUGGINGFACE_HUB_TOKEN"] = hf_token
        print("HF token found and set")
    else:
        print("WARNING: No HF token found in environment")
        # Continue with public model instead of failing
    # Ensure we're using the right Python
    PYTHON_PATH = "/opt/conda/envs/llm-foundry/bin/python"
    
    # Use the correct Python interpreter for imports
    import_check = subprocess.run(
        [PYTHON_PATH, "-c", "import flash_attn; print(flash_attn.__version__)"],
        capture_output=True,
        text=True
    )
    print(f"Flash Attention version: {import_check.stdout}")



    # ############ Working steps start############
    # # Step 0: Test Llama implementation: load and generate
    # yaml_path = "/llm-foundry/scripts/train/yamls/llama/llama3-1b-lora.yaml"
    # print(f"Loading config from {yaml_path}")
    # cfg = OmegaConf.load(yaml_path)
    # print("Config loaded successfully")
    
    # # Pass config to build_llama function
    # model = build_llama(cfg)
    
    # # Pass config to llama_generate function
    # llama_generate(model, cfg)
    # ############ Working steps end############
    
    # Create entry script
    #entry_script = create_entry_script()

    # Update YAML
    yaml_path = "/llm-foundry/scripts/train/yamls/llama/llama3-1b-lora.yaml"

    # Run using our entry script
    # os.chdir("/llm-foundry/scripts")
    # print("\nRunning training with custom model...")
    # train_cmd = [PYTHON_PATH, entry_script, yaml_path, "/root/c4-data"]
    # result = subprocess.run(train_cmd, capture_output=True, text=True)
    # print(result.stdout)
    
    # Step 1: Prepare data - use C4 TODO: Clean up args to load from yaml
    print("\nPreparing data...")
    os.chdir("/llm-foundry/scripts")
    data_prep_cmd = [
        PYTHON_PATH,
        "data_prep/convert_dataset_hf.py",
        "--dataset", "allenai/c4",
        "--data_subset", "en",
        "--out_root", "/root/c4-data",
        "--splits", "train_small", "val_small",  # Separate arguments, not comma-separated
        "--concat_tokens", "2048",
        "--tokenizer", "meta-llama/Llama-3.2-1B",
        "--eos_text", "<|endoftext|>",
        #"--timeout", "60"  # Increase timeout to 60 seconds

    ]
    result = subprocess.run(data_prep_cmd, capture_output=True, text=True)
    print(result.stdout)
    if result.stderr:
        print("Data prep errors:", result.stderr)
    
    # Step 2: Fine-tune model with LoRA
    # print("\nFine-tuning Llama-3 with LoRA...")
    # yaml_path = os.path.join("/llm-foundry/scripts/train/yamls/llama", "llama3-1b-lora.yaml")
    # #check_and_create_custom_plugin()

    # # Modify build_composer_model to automatically import our plugin
    # #modify_build_composer_model()
    # os.chdir("/llm-foundry/scripts") 
    # train_cmd = [
    #     "composer",
    #     #"--import_module", "train.custom_plugin.model_registry",  # Import our module
    #     "train/train.py",
    #     yaml_path,
    #     "variables.data_local=/root/c4-data",
    #     #"model.name=llama3_1b"  # Override to use our registered model
    # ]
    # print(f"Running command: {' '.join(train_cmd)}")
    # result = subprocess.run(train_cmd, capture_output=True, text=True)
    # print(result.stdout)
    # if result.stderr:
    #     print("Training errors:", result.stderr)
    


    # Create the combined script
    combined_script = create_combined_script()

    # Run it directly
    print("\nRunning combined training script...")
    os.chdir("/llm-foundry/scripts")
    combined_cmd = [
        PYTHON_PATH,
        combined_script,
        yaml_path,
        "/root/c4-data"
    ]
    result = subprocess.run(combined_cmd, capture_output=True, text=True)
    print(result.stdout)
    if result.stderr:
        print("Combined script errors:", result.stderr)



    # Step 3: Convert model to HuggingFace format
    print("\nConverting model to HuggingFace format...")
    convert_cmd = [
        PYTHON_PATH, "inference/convert_composer_to_hf.py",
        "--composer_path", "/root/llama3-c4/latest-rank0.pt",  
        "--hf_output_path", "/root/llama3-c4-hf",
        "--output_precision", "bf16"
    ]
    result = subprocess.run(convert_cmd, capture_output=True, text=True)
    print(result.stdout)
    if result.stderr:
        print("Conversion errors:", result.stderr)
    
    # Step 4: Evaluate the model on math problems
    print("\nEvaluating model on math problems...")
    # eval_cmd = [
    #     "composer",
    #     "eval/eval.py",
    #     "eval/yamls/hf_eval.yaml",
    #     "icl_tasks=eval/yamls/tasks/c4.yaml",
    #     "variables.model_name_or_path=/root/llama3-c4-hf"
    # ]
    # result = subprocess.run(eval_cmd, capture_output=True, text=True)
    # print(result.stdout)
    # if result.stderr:
    #     print("Evaluation errors:", result.stderr)
    
    model_path = "/root/llama3-c4-hf"
    save_path = "/root/eval_results"
    os.makedirs(save_path, exist_ok=True)
    eval_cmd = [
    "composer",
    "eval/eval.py",
    "eval/yamls/hf_eval.yaml",
    "icl_tasks=eval/yamls/copa.yaml",
    f"variables.model_name_or_path={model_path}",
    f"results_path={save_path}",  # Add results_path parameter
    ]
    result = subprocess.run(eval_cmd, capture_output=True, text=True)
    print(result.stdout)
    if result.stderr:
        print("Evaluation errors:", result.stderr)
    
    # Step 5: Generate test responses for math problems
    print("\nGenerating test responses for math problems...")
    # generate_cmd = [
    #     PYTHON_PATH, "inference/hf_generate.py",
    #     "--name_or_path", "/root/llama3-c4-hf",
    #     "--max_new_tokens", "256",
    #     "--temperature", "0.1",
    #     "--top_p", "0.9",
    #     "--prompts",
    #     "Question: If a shirt originally costs $25 and is marked down by 20%, what is the new price of the shirt? Answer:",
    #     "Question: A train travels at a speed of 60 mph. How far will it travel in 3.5 hours? Answer:"
    # ]
    #     print("\nGenerating test responses...")
    prompts = None
    if prompts is None:
        prompts = [
            "The answer to life, the universe, and happiness is",
            "Here's a quick recipe for baking chocolate chip cookies: Start by",
        ]
    elif isinstance(prompts, str):
        prompts = [prompts]
    

    print("\nGenerating test responses...")
    generate_cmd = [
        PYTHON_PATH, "inference/hf_generate.py",
        "--name_or_path", model_path,
        "--max_new_tokens", "256",
        "--prompts",
        *prompts,
    ]
    result = subprocess.run(generate_cmd, capture_output=True, text=True)
    print(result.stdout)
    if result.stderr:
        print("Generation errors:", result.stderr)
    print("Generation complete!")

    result = subprocess.run(generate_cmd, capture_output=True, text=True)
    print(result.stdout)
    if result.stderr:
        print("Generation errors:", result.stderr)

    return "Llama training and evaluation completed!"

@app.local_entrypoint()
def main():
    run_llama.remote()