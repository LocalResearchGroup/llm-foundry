# llama3-1b.yaml
variables:
  data_local: /root/c4-data
  data_remote: 
  tokenizer_name: meta-llama/Llama-3.2-1B
  global_seed: 17
  max_seq_len: 2048
  run_name: llama3-lora-test
  model_name_or_path: meta-llama/Llama-3.2-1B
  #name_or_path: meta-llama/Llama-3.2-1B

max_seq_len: ${variables.max_seq_len}
run_name: ${variables.run_name}


model:
  name: hf_causal_lm #llama3_1b 
  #import_path: train.custom_plugin.model_registry #train.custom_plugin.model_registry 
  #import_path: scripts.train.model_registry 
  #import_path: llmfoundry.models.llama  # Points to your custom module
  pretrained_model_name_or_path: ${variables.model_name_or_path}
  use_flash_attention_2: true
  peft_config:
    peft_type: LORA  # Uppercase
    task_type: CAUSAL_LM
    r: 8
    lora_alpha: 16  # Added 'lora_' prefix
    lora_dropout: 0.05  # Added 'lora_' prefix
    # YAML list format
    target_modules:  
      - q_proj
      - k_proj
      - v_proj
      - o_proj
    task_type: CAUSAL_LM
# Tokenizer
tokenizer:
  name: ${variables.tokenizer_name}
  kwargs:
    model_max_length: ${variables.max_seq_len}

# Data
train_loader:
  name: text
  dataset:
    local: ${variables.data_local}
    remote: ${variables.data_remote}
    split: train_small
    shuffle: true
    max_seq_len: ${variables.max_seq_len}
    shuffle_seed: ${variables.global_seed}
  drop_last: true
  num_workers: 1

eval_loader:
  name: text
  dataset:
    local: ${variables.data_local}
    remote: ${variables.data_remote}
    split: val_small
    shuffle: false
    max_seq_len: ${variables.max_seq_len}
    shuffle_seed: ${variables.global_seed}
  drop_last: false
  num_workers: 1



optimizer:
  name: decoupled_adamw
  lr: 6.0e-4
  betas:
  - 0.9
  - 0.95
  eps: 1.0e-08
  weight_decay: 0.0

scheduler:
  name: cosine_with_warmup
  t_warmup: 100ba
  alpha_f: 0.1
# scheduler:
#   name: constant
#   interval: step



algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0

max_duration: 10ba
eval_interval: 0
eval_first: false
eval_subset_num_batches: -1
global_train_batch_size: 1

# System
seed: ${variables.global_seed}
device_eval_batch_size: 1
device_train_microbatch_size: 1
precision: amp_bf16

# Saving
save_folder: /root/llama3-c4
save_interval: 5ba

# Logging
progress_bar: true
log_to_console: true
console_log_interval: 1ba

callbacks:
  speed_monitor:
    window_size: 10
  lr_monitor: {}
  memory_monitor: {}


# max_duration: 10ba
# eval_interval: 0
# save_folder: /root/llama3-c4
# save_interval: 5ba
# global_train_batch_size: 1
# device_train_microbatch_size: 1
# device_eval_batch_size: 1
# precision: amp_bf16

#TODO: add loggers!