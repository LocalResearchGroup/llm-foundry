# llama3-1b.yaml
model:
  name: llama3-1b
  import_path: llmfoundry.models.llama
  d_model: 2048  # hidden_size
  n_heads: 16    # num_attention_heads
  n_kv_heads: 4  # num_key_value_heads  
  n_layers: 22   # num_hidden_layers
  expansion_ratio: 4  # for intermediate_size (4 * d_model = 8192)
  max_seq_len: 8192
  vocab_size: 128256
  rope_theta: 500000.0
  rms_norm_eps: 1e-5
  use_unpadded_rope: true
  use_flash_attn: true
  
  # For weight loading
  pretrained_model_name_or_path: meta-llama/Llama-3.2-1B
  
  # For PEFT fine-tuning
  peft:
    peft_method: lora  # or dora, rslora
    r: 8
    alpha: 16
    dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    bias: none
    task_type: CAUSAL_LM